{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split \n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sonar.csv')\n",
    "\n",
    "X = dataset.iloc[:, :-1] \n",
    "y = dataset.iloc[:, -1]  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into Training and Test groups (use 20-80 split, i.e. 20% of data will be used for the Test group and 80% for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression \n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier (with nb_trees = 10)\n",
    "- K- Nearest Neighbors (K-NN)\n",
    "- Support Vector Machine (SVM) – use the ‘rbf’ kernel\n",
    "\n",
    "Train each of the above models and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Logistic Regression Decision Tree Random Forest  K-NN Naive Bayes  \\\n",
      "161   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "15    Rock                Rock          Mine          Mine  Rock        Rock   \n",
      "73    Rock                Mine          Rock          Rock  Rock        Rock   \n",
      "96    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "166   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "9     Rock                Mine          Mine          Rock  Rock        Rock   \n",
      "100   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "135   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "18    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "148   Mine                Rock          Mine          Mine  Rock        Mine   \n",
      "171   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "30    Rock                Rock          Mine          Rock  Rock        Rock   \n",
      "155   Mine                Rock          Rock          Rock  Mine        Rock   \n",
      "180   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "125   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "197   Mine                Mine          Mine          Mine  Mine        Rock   \n",
      "164   Mine                Mine          Mine          Mine  Mine        Rock   \n",
      "190   Mine                Mine          Mine          Mine  Mine        Rock   \n",
      "84    Rock                Rock          Rock          Mine  Rock        Rock   \n",
      "75    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "124   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "170   Mine                Mine          Rock          Rock  Mine        Rock   \n",
      "104   Mine                Rock          Mine          Mine  Mine        Mine   \n",
      "101   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "69    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "25    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "95    Rock                Rock          Mine          Rock  Rock        Rock   \n",
      "16    Rock                Rock          Mine          Mine  Mine        Mine   \n",
      "141   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "185   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "154   Mine                Rock          Rock          Mine  Rock        Rock   \n",
      "68    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "66    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "120   Mine                Mine          Rock          Mine  Mine        Rock   \n",
      "147   Mine                Mine          Rock          Mine  Mine        Mine   \n",
      "98    Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "138   Mine                Mine          Mine          Mine  Mine        Mine   \n",
      "167   Mine                Mine          Rock          Rock  Rock        Rock   \n",
      "45    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "113   Mine                Rock          Mine          Mine  Mine        Rock   \n",
      "65    Rock                Rock          Rock          Rock  Rock        Rock   \n",
      "178   Mine                Rock          Rock          Rock  Mine        Rock   \n",
      "\n",
      "      SVM  \n",
      "161  Mine  \n",
      "15   Rock  \n",
      "73   Rock  \n",
      "96   Rock  \n",
      "166  Mine  \n",
      "9    Rock  \n",
      "100  Mine  \n",
      "135  Mine  \n",
      "18   Rock  \n",
      "148  Mine  \n",
      "171  Mine  \n",
      "30   Rock  \n",
      "155  Rock  \n",
      "180  Mine  \n",
      "125  Mine  \n",
      "197  Mine  \n",
      "164  Mine  \n",
      "190  Mine  \n",
      "84   Rock  \n",
      "75   Rock  \n",
      "124  Mine  \n",
      "170  Mine  \n",
      "104  Mine  \n",
      "101  Mine  \n",
      "69   Rock  \n",
      "25   Rock  \n",
      "95   Rock  \n",
      "16   Mine  \n",
      "141  Mine  \n",
      "185  Mine  \n",
      "154  Rock  \n",
      "68   Rock  \n",
      "66   Rock  \n",
      "120  Mine  \n",
      "147  Mine  \n",
      "98   Mine  \n",
      "138  Mine  \n",
      "167  Rock  \n",
      "45   Rock  \n",
      "113  Mine  \n",
      "65   Rock  \n",
      "178  Rock  \n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(random_state=42)\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "random_forest = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "naive_bayes = GaussianNB()\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "decision_tree.fit(X_train_scaled, y_train)\n",
    "random_forest.fit(X_train_scaled, y_train)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "naive_bayes.fit(X_train_scaled, y_train)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_logistic_regression = logistic_regression.predict(X_test_scaled)\n",
    "y_pred_decision_tree = decision_tree.predict(X_test_scaled)\n",
    "y_pred_random_forest = random_forest.predict(X_test_scaled)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "y_pred_naive_bayes = naive_bayes.predict(X_test_scaled)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Logistic Regression': y_pred_logistic_regression,\n",
    "    'Decision Tree': y_pred_decision_tree,\n",
    "    'Random Forest': y_pred_random_forest,\n",
    "    'K-NN': y_pred_knn,\n",
    "    'Naive Bayes': y_pred_naive_bayes,\n",
    "    'SVM': y_pred_svm\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and print the Confusion Matrix and the accuracy scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Logistic Regression =====\n",
      "Confusion Matrix:\n",
      "[[20  6]\n",
      " [ 2 14]]\n",
      "Accuracy Score: 0.8095\n",
      "\n",
      "===== Decision Tree =====\n",
      "Confusion Matrix:\n",
      "[[19  7]\n",
      " [ 5 11]]\n",
      "Accuracy Score: 0.7143\n",
      "\n",
      "===== Random Forest =====\n",
      "Confusion Matrix:\n",
      "[[22  4]\n",
      " [ 3 13]]\n",
      "Accuracy Score: 0.8333\n",
      "\n",
      "===== K-NN =====\n",
      "Confusion Matrix:\n",
      "[[23  3]\n",
      " [ 1 15]]\n",
      "Accuracy Score: 0.9048\n",
      "\n",
      "===== Naive Bayes =====\n",
      "Confusion Matrix:\n",
      "[[16 10]\n",
      " [ 1 15]]\n",
      "Accuracy Score: 0.7381\n",
      "\n",
      "===== SVM =====\n",
      "Confusion Matrix:\n",
      "[[22  4]\n",
      " [ 1 15]]\n",
      "Accuracy Score: 0.8810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_evaluation(model_name, y_true, y_pred):\n",
    "    print(f\"===== {model_name} =====\") \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    " \n",
    "    acc_score = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy Score: {acc_score:.4f}\\n\")\n",
    " \n",
    "print_evaluation(\"Logistic Regression\", y_test, y_pred_logistic_regression)\n",
    "print_evaluation(\"Decision Tree\", y_test, y_pred_decision_tree)\n",
    "print_evaluation(\"Random Forest\", y_test, y_pred_random_forest)\n",
    "print_evaluation(\"K-NN\", y_test, y_pred_knn)\n",
    "print_evaluation(\"Naive Bayes\", y_test, y_pred_naive_bayes)\n",
    "print_evaluation(\"SVM\", y_test, y_pred_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, use the K-fold cross-validation (use K=10; in python - cv=10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Logistic Regression (K-fold Cross-Validation) =====\n",
      "Accuracy Scores: [0.76470588 0.52941176 1.         0.82352941 0.76470588 0.70588235\n",
      " 0.8125     0.5625     0.875      0.8125    ]\n",
      "Mean Accuracy: 0.7651\n",
      "Standard Deviation: 0.1324\n",
      "\n",
      "===== Decision Tree (K-fold Cross-Validation) =====\n",
      "Accuracy Scores: [0.88235294 0.70588235 0.64705882 0.70588235 0.76470588 0.70588235\n",
      " 0.875      0.6875     0.75       0.8125    ]\n",
      "Mean Accuracy: 0.7537\n",
      "Standard Deviation: 0.0758\n",
      "\n",
      "===== Random Forest (K-fold Cross-Validation) =====\n",
      "Accuracy Scores: [0.82352941 0.76470588 0.70588235 0.82352941 0.88235294 0.76470588\n",
      " 0.6875     0.8125     0.75       0.8125    ]\n",
      "Mean Accuracy: 0.7827\n",
      "Standard Deviation: 0.0563\n",
      "\n",
      "===== K-NN (K-fold Cross-Validation) =====\n",
      "Accuracy Scores: [0.94117647 0.58823529 0.70588235 0.82352941 0.64705882 0.76470588\n",
      " 0.8125     0.8125     0.875      0.75      ]\n",
      "Mean Accuracy: 0.7721\n",
      "Standard Deviation: 0.0997\n",
      "\n",
      "===== Naive Bayes (K-fold Cross-Validation) =====\n",
      "Accuracy Scores: [0.82352941 0.64705882 0.88235294 0.58823529 0.58823529 0.76470588\n",
      " 0.625      0.625      0.8125     0.5625    ]\n",
      "Mean Accuracy: 0.6919\n",
      "Standard Deviation: 0.1107\n",
      "\n",
      "===== SVM (K-fold Cross-Validation) =====\n",
      "Accuracy Scores: [0.94117647 0.76470588 0.94117647 0.82352941 0.82352941 0.88235294\n",
      " 0.8125     0.8125     0.8125     0.8125    ]\n",
      "Mean Accuracy: 0.8426\n",
      "Standard Deviation: 0.0561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cross_val_and_print(model, model_name): \n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=10, scoring='accuracy')\n",
    " \n",
    "    print(f\"===== {model_name} (K-fold Cross-Validation) =====\")\n",
    "    print(f\"Accuracy Scores: {scores}\")\n",
    "    print(f\"Mean Accuracy: {scores.mean():.4f}\")\n",
    "    print(f\"Standard Deviation: {scores.std():.4f}\\n\")\n",
    " \n",
    "cross_val_and_print(logistic_regression, \"Logistic Regression\")\n",
    "cross_val_and_print(decision_tree, \"Decision Tree\")\n",
    "cross_val_and_print(random_forest, \"Random Forest\")\n",
    "cross_val_and_print(knn, \"K-NN\")\n",
    "cross_val_and_print(naive_bayes, \"Naive Bayes\")\n",
    "cross_val_and_print(svm, \"SVM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the chosen best performing model select two hyperparameters you would like to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'var_smoothing': 1e-09}\n",
      "Best Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "gnb = GaussianNB()\n",
    " \n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "}\n",
    " \n",
    "grid_search = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    " \n",
    "grid_search.fit(X_train, y_train)\n",
    " \n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Accuracy: {:.2f}\".format(grid_search.best_score_))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameter, we can see that the model performed slightly better, however the value of var_smoothing is the same as the default one used - 1e-09"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
